{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "coursera": {
      "course_slug": "neural-networks-deep-learning",
      "graded_item_id": "wRuwL",
      "launcher_item_id": "NI888"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "Assignment3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0y5iCjR0m__"
      },
      "source": [
        "# Artificial Neural Networks\n",
        "Welcome to the third assignment! In this assignment, you will make a 2 layer neural network for data classification where we will be dividing the regions between red and blue. Through this assignment, you will go through all the steps required for building a shallow 2 layered neural network consisting of 1 hidden layer and 1 output layer. \n",
        "\n",
        "**You will learn to:**\n",
        "- This assignment will help you in building the general architecture of your first neural network with 1 hidden layer and steps include:\n",
        "    - Initializing parameters\n",
        "    - Forward Propagation\n",
        "    - Backward Propagation\n",
        "    - Gradient descent to improve the algorithm \n",
        "- Collect all the above functions into a single function in the correct order and this function will act as a main model.\n",
        "\n",
        "**NOTE:** Many software bugs in deep learning come from having matrix/vector dimensions that don't fit. If you can keep your matrix/vector dimensions straight you will go a long way toward eliminating many bugs. \n",
        "\n",
        "### Recommended Reading\n",
        "\n",
        "-   [Chapter 3](https://livebook.manning.com/book/deep-learning-with-python/chapter-3) of \"Deep Learning with Python\" by F. Chollet. \n",
        "- [Artificial Neural networks](https://medium.com/towards-artificial-intelligence/building-neural-networks-from-scratch-with-python-code-and-math-in-detail-i-536fae5d7bbf) \n",
        "- [Neural networks and Backpropagation](https://medium.com/datathings/neural-networks-and-backpropagation-explained-in-a-simple-way-f540a3611f5e)\n",
        "- Building Neural Network from [scratch](https://towardsdatascience.com/coding-a-2-layer-neural-network-from-scratch-in-python-4dd022d19fd2)\n",
        "-  [Activation functions](https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6) - Sigmoid, tanh, Softmax, ReLU, Leaky ReLU\n",
        "- Artificial neural networks [explained](http://scs.ryerson.ca/~aharley/neural-networks/)\n",
        "- Building a 2 layered neural network for [3 class classification problem](https://cs231n.github.io/neural-networks-case-study/) \n",
        "\n",
        "#### Derivatives and Gradients\n",
        "\n",
        "- https://www.mathsisfun.com/calculus/derivatives-introduction.html\n",
        "- (chapters 3-4) https://openstax.org/books/calculus-volume-1/pages/3-1-defining-the-derivative#27277\n",
        "\n",
        "### Additional Material (Optional!)\n",
        "\n",
        "- how ANN can [represent complex functions](http://neuralnetworksanddeeplearning.com/chap4.html) \n",
        "- <a href='http://faroit.com/keras-docs/2.0.2/getting-started/faq/#what-does-sample-batch-epoch-mean'>What does \"sample\", \"batch\", \"epoch\" mean? From Keras FAQ </a>\n",
        "\n",
        "\n",
        "**efficiently computing gradients via backpropagation:**\n",
        "- intro level [3blue1brown educational series](https://www.youtube.com/watch?v=Ilg3gGewQ5U)\n",
        "- more in-depth explanation [cs231n Stanford course](https://www.youtube.com/watch?v=d14TUNcbn1k)\n",
        "\n",
        "\n",
        "**Regression task with ANN**\n",
        "\n",
        "- Deep Learning with Python F.Chollet, [chapter 3.6](https://livebook.manning.com/book/deep-learning-with-python/chapter-3/271)\n",
        "\n",
        "- how ANN can [represent complex functions](http://neuralnetworksanddeeplearning.com/chap4.html) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OInY2q91G38A"
      },
      "source": [
        "Let's get started\n",
        "\n",
        "\n",
        "**First of all, mount google drive**\n",
        "\n",
        "This will mount the google drive for google colab and you will be able access contents of your drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkoN2ObcG8e7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive/Deep_learning_unit/Assignment3')\n",
        "folder = os.path.join('/content/drive/My Drive/Deep_learning_unit/Assignment3')\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcGLlswS0nAD"
      },
      "source": [
        "## 1 - Packages ##\n",
        "\n",
        "First, let's run the cell below to import all the packages that you will need during this exercise. \n",
        "- [numpy](https://www.numpy.org/) is the fundamental package for scientific computing with Python.\n",
        "- [sklearn](http://scikit-learn.org/stable/) provides simple and efficient tools for data mining and data analysis. \n",
        "- [matplotlib](http://matplotlib.org) is a library for plotting graphs in Python.\n",
        "- testCases provides some test examples to assess the correctness of your functions\n",
        "- planar_utils provide various useful functions used in this assignment\n",
        "\n",
        "- Set a seed using np.random.seed(1) so that the results are consistent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0865EDO0nAF"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from testCases_v2 import *\n",
        "import sklearn\n",
        "import sklearn.datasets\n",
        "import sklearn.linear_model\n",
        "from planar_utils import plot_decision_boundary, sigmoid, load_datasets\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "np.random.seed(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYkWFwfn0nAS"
      },
      "source": [
        "## 2 - Noisy moons dataset ##\n",
        "\n",
        "Noisy moons dataset displays 2 disjunctive clusters of data points in a 2-dimensional representation space ( with coordinates x1 and x2 for two features). You will design the simple neural nwtwork to classify the data points in two classes blue or red. \n",
        "\n",
        "First, let's get the dataset you will work on. The following code will load a \"Noisy moons\" 2-class dataset into variables `X` and `Y`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVJi9JUa0nAT"
      },
      "source": [
        "#noisy_moons = sklearn.datasets.make_moons(n_samples=400, noise=.2)\n",
        "#X, Y = noisy_moons\n",
        "noisy_moons = load_datasets()\n",
        "\n",
        "datasets = {\"noisy_moons\": noisy_moons}\n",
        "\n",
        "X, Y = datasets[\"noisy_moons\"]\n",
        "X, Y = X.T, Y.reshape(1, Y.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0LWP7yV0nAY"
      },
      "source": [
        "Visualize the dataset using matplotlib. The data looks like a \"two cresecent moons\" with some red (label y=0) and some blue (y=1) points. You will build a classifier to fit this data by defining the regions as either red or blue."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7RV0teH0nAc"
      },
      "source": [
        "# Visualize the data:\n",
        "plt.scatter(X[0, :], X[1, :], c=Y, s=40, cmap=plt.cm.Spectral);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGRZcKab0nAf"
      },
      "source": [
        "You have:\n",
        "    - a numpy-array (matrix) X that contains your features (x1, x2)\n",
        "    - a numpy-array (vector) Y that contains your labels (red:0, blue:1).\n",
        "\n",
        "Calculate the number of training examples? In addition, what is the `shape` of the variables `X` and `Y`? \n",
        "\n",
        "\n",
        "Hint: You can use the shape function to get the above values. m represents the number of training examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBjHRxjR0nAg"
      },
      "source": [
        "### START CODE HERE ### (â‰ˆ 3 lines of code)\n",
        "shape_X = \n",
        "shape_Y = \n",
        "m =   \n",
        "### END CODE HERE ###\n",
        "\n",
        "print ('The shape of X is: ' + str(shape_X))\n",
        "print ('The shape of Y is: ' + str(shape_Y))\n",
        "print ('I have m = %d training examples!' % (m))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8y7iX1l0nAj"
      },
      "source": [
        "**Expected Output**:\n",
        "       \n",
        "<table style=\"width:20%\">\n",
        "  <tr>\n",
        "    <td>**shape of X**</td>\n",
        "    <td> (2, 400) </td> \n",
        "  </tr> \n",
        "  <tr>\n",
        "    <td>**shape of Y**</td>\n",
        "    <td>(1, 400) </td> \n",
        "  </tr>\n",
        "     <tr>\n",
        "    <td>**m**</td>\n",
        "    <td> 400 </td> \n",
        "  </tr>\n",
        "  \n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxLkMNjA0nAk"
      },
      "source": [
        "## 3 - Try Logistic Regression\n",
        "\n",
        "Let's try logistic regression model before building a full neural network. We will be using sklearn's built-in function to do that. Test the logistic regression model for the classification of the selected dataset and plot the decision boundary using this classifier. The code below trains the logictic regression classifier:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2Fm1iJA0nAl"
      },
      "source": [
        "clf = sklearn.linear_model.LogisticRegressionCV();\n",
        "clf.fit(X.T, Y.T);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9B2vLtByKpNl"
      },
      "source": [
        "Code below plots the decision boundary for logistic regression and also calculates the accuracy of the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "LZgiHuK20nAn"
      },
      "source": [
        "# Plot the decision boundary for logistic regression\n",
        "plot_decision_boundary(lambda x: clf.predict(x), X, Y)\n",
        "plt.title(\"Logistic Regression\")\n",
        "\n",
        "# Print accuracy\n",
        "LR_predictions = clf.predict(X.T)\n",
        "print ('Accuracy of logistic regression: %d ' % float((np.dot(Y,LR_predictions) + np.dot(1-Y,1-LR_predictions))/float(Y.size)*100) +\n",
        "       '% ' + \"(percentage of correctly labelled datapoints)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8eZU-h70nAp"
      },
      "source": [
        "**Expected Output**:\n",
        "\n",
        "<table style=\"width:20%\">\n",
        "  <tr>\n",
        "    <td>**Accuracy**</td>\n",
        "    <td> 86% </td> \n",
        "  </tr> \n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pLs14Q20nAt"
      },
      "source": [
        "**Interpretation**: As you can see that some of the data points are non-linearly separable, hence we need an improved model to better define the boundary of our dataset. \n",
        "\n",
        "Now, we will try neural network for the same problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75pPtsGcMBPP"
      },
      "source": [
        "## 4 - Neural Network model\n",
        "\n",
        "We are going to build a 2 layered neural network with 1 hidden layer. You will learn how to:\n",
        "\n",
        "**Step 1 - \"Initialization\".** Choose a first guess (e.g. using random number generators) for the ANN weights.\n",
        "\n",
        "**Step 2 - \"Forward Pass\".** Compute the predicted labels $\\hat{y}^{(i)}$ for a randomly chosen subset (batch) of the training set. \n",
        "\n",
        "**Step 3 - \"Backward Pass\".** Compute loss and calculate derivative for the gradient of the loss for the resulting prediction errors ${y}^{(i)} - \\hat y^{(i)}$.  \n",
        "\n",
        "**Step 4 - \"Gradient Step\".** Update the ANN weights by taking a (small) step into the opposite direction of the gradient: $\\mathbf{w} \\mapsto \\mathbf{w} - \\alpha * d(\\mathbf{w})$. \n",
        "\n",
        "See the figure below for schematic representation of these steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVPIMeAhL-rl"
      },
      "source": [
        "plt.figure(figsize=(20,8))\n",
        "img=plt.imread('/content/drive/My Drive/Deep_learning_unit/Assignment3/images/ANN_cycle.png')\n",
        "plt.imshow(img)\n",
        "plt.axis('off') \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pou7iovrU_S5"
      },
      "source": [
        "**NEURAL NETWORK ARCHITECTURE**\n",
        "\n",
        "The general methodology to build a Neural Network is to:\n",
        "  1. Define the neural network structure ( # of input units,  # of hidden units, etc). \n",
        "  2. Initialize the model's parameters\n",
        "  3. Loop:\n",
        "        - Implement forward propagation\n",
        "        - Compute loss\n",
        "        - Implement backward propagation to get the gradients\n",
        "        - Update parameters (gradient descent)\n",
        "\n",
        "You often build helper functions to compute steps 1-3 and then merge them into one function we call `nn_model()`. Once you've built `nn_model()` and learnt the right parameters, you can make predictions on new data.\n",
        "\n",
        "Let us define the neural network architecture and see how our model look look like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ba1DtX_6VA1v"
      },
      "source": [
        "plt.figure(figsize=(20,8))\n",
        "img=plt.imread('/content/drive/My Drive/Deep_learning_unit/Assignment3/images/classification_kiank.png')\n",
        "plt.imshow(img)\n",
        "plt.axis('off') \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRKychJM0nAu"
      },
      "source": [
        "### 4.1 - Neural network structure ####\n",
        "We design an ANN with 3 layers: 1 input layer, 1 hidden layer and 1 output layer but often refereed as 2 layered Neural network (2 layers counted are hidden and output). The hidden layers are called \"hidden\", because in contrast to input and output layers we cannot directly access the inputs and outputs of the hidden layers.\n",
        "\n",
        " **input layer**. As customary in the deep learning literature, we refer to the input features $x_{1},\\ldots,x_{n}$ of a data point as the input layer. The example below includes 2 features values $x_{1},x_{2}$.\n",
        "\n",
        "**layer 1 - hidden layer** This layer consists of 4 neurons. Each neuron in this layer receives the input from all neurons of the previous layer. This type of layers are called fully connected or densely connected layer or **dense layer**.\n",
        "\n",
        "**layer 2 - output layer** This layer consists of one output neuron. The output neuron represents a particular class and the output value represents the confidence in assigning the data point to the class. \n",
        "\n",
        "We use the following variables for defining the number of nodes in each of the layer:\n",
        "  - n_x: the size of the input layer\n",
        "  - n_h: the size of the hidden layer\n",
        "  - n_y: the size of the output layer\n",
        "\n",
        "**Hint**: Use shapes of X and Y to find n_x and n_y. We will use the number of nodes in hidden layer as 4 hard code the hidden layer size to be 4."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnKTr1Vb0nAu"
      },
      "source": [
        "def layer_sizes(X, Y):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    X -- input dataset of shape (input size, number of examples)\n",
        "    Y -- labels of shape (output size, number of examples)\n",
        "    \n",
        "    Returns:\n",
        "    n_x -- the size of the input layer\n",
        "    n_h -- the size of the hidden layer\n",
        "    n_y -- the size of the output layer\n",
        "    \"\"\"\n",
        "    ### START CODE HERE ### (â‰ˆ 3 lines of code)\n",
        "  \n",
        "\n",
        "  \n",
        "    ### END CODE HERE ###\n",
        "    return (n_x, n_h, n_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-MEiflX0nAw"
      },
      "source": [
        "(n_x, n_h, n_y) = layer_sizes(X, Y)\n",
        "print(\"The size of the input layer is: n_x = \" + str(n_x))\n",
        "print(\"The size of the hidden layer is: n_h = \" + str(n_h))\n",
        "print(\"The size of the output layer is: n_y = \" + str(n_y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQ0dUDKC0nAy"
      },
      "source": [
        "**Expected Output**\n",
        "\n",
        "<table style=\"width:20%\">\n",
        "  <tr>\n",
        "    <td>n_x</td>\n",
        "    <td> 2 </td> \n",
        "  </tr> \n",
        "    <tr>\n",
        "    <td>n_h</td>\n",
        "    <td> 4 </td> \n",
        "  </tr> \n",
        "    <tr>\n",
        "    <td>n_y</td>\n",
        "    <td> 1 </td> \n",
        "  </tr>\n",
        "  \n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DX91YtqC0nAy"
      },
      "source": [
        "### 4.2 - Parameter initialization ####\n",
        "\n",
        "- You will initialize the weights matrices with random values. \n",
        "    - Use: `np.random.randn(a,b) * 0.01` to randomly initialize a matrix of shape (a,b).\n",
        "- You will initialize the bias vectors as zeros. \n",
        "    - Use: `np.zeros((a,b))` to initialize a matrix of shape (a,b) with zeros.\n",
        "\n",
        "Make sure your parameters' sizes are right. Refer to the neural network figure above if required.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96fBBYM90nAy"
      },
      "source": [
        "def initialize_parameters(n_x, n_h, n_y):\n",
        "    \"\"\"\n",
        "    Argument:\n",
        "    n_x -- size of the input layer\n",
        "    n_h -- size of the hidden layer\n",
        "    n_y -- size of the output layer\n",
        "    \n",
        "    Returns:\n",
        "    params -- python dictionary containing your parameters:\n",
        "                    W1 -- weight matrix of shape (n_h, n_x)\n",
        "                    b1 -- bias vector of shape (n_h, 1)\n",
        "                    W2 -- weight matrix of shape (n_y, n_h)\n",
        "                    b2 -- bias vector of shape (n_y, 1)\n",
        "    \"\"\"\n",
        "    \n",
        "    np.random.seed(2) # we set up a seed so that your output matches ours although the initialization is random.\n",
        "    \n",
        "    ### START CODE HERE ### (â‰ˆ 4 lines of code)\n",
        "   \n",
        "\n",
        "\n",
        "   \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    assert (W1.shape == (n_h, n_x))\n",
        "    assert (b1.shape == (n_h, 1))\n",
        "    assert (W2.shape == (n_y, n_h))\n",
        "    assert (b2.shape == (n_y, 1))\n",
        "    \n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "    \n",
        "    return parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_se5bdBB0nA0"
      },
      "source": [
        "n_x, n_h, n_y = initialize_parameters_test_case()\n",
        "\n",
        "parameters = initialize_parameters(n_x, n_h, n_y)\n",
        "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
        "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
        "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
        "print(\"b2 = \" + str(parameters[\"b2\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENap8v1j0nA1"
      },
      "source": [
        "**Expected Output**:\n",
        "\n",
        "<table style=\"width:90%\">\n",
        "  <tr>\n",
        "    <td>**W1**</td>\n",
        "    <td> [[-0.00416758 -0.00056267]\n",
        " [-0.02136196  0.01640271]\n",
        " [-0.01793436 -0.00841747]\n",
        " [ 0.00502881 -0.01245288]] </td> \n",
        "  </tr>\n",
        "  \n",
        "  <tr>\n",
        "    <td>**b1**</td>\n",
        "    <td> [[ 0.]\n",
        " [ 0.]\n",
        " [ 0.]\n",
        " [ 0.]] </td> \n",
        "  </tr>\n",
        "  \n",
        "  <tr>\n",
        "    <td>**W2**</td>\n",
        "    <td> [[-0.01057952 -0.00909008  0.00551454  0.02292208]]</td> \n",
        "  </tr>\n",
        "  \n",
        "\n",
        "  <tr>\n",
        "    <td>**b2**</td>\n",
        "    <td> [[ 0.]] </td> \n",
        "  </tr>\n",
        "  \n",
        "</table>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77KN-DqY0nA2"
      },
      "source": [
        "### 4.3 - Forward propagation ####\n",
        "The forward propagation calculates the $Z^{[1]}$, $A^{[1]}$ and $Z^{[2]}$ $A^{[2]}$ for the hidden layer and the output layer. Mathematically, they can be represented as:\n",
        "\n",
        "$x^{(i)}$:\n",
        "$$Z^{[1]} =  W^{[1]} X + b^{[1]}\\tag{1}$$ \n",
        "$$A^{[1]} = \\tanh(Z^{[1]})\\tag{2}$$\n",
        "$$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}\\tag{3}$$\n",
        "$$\\hat{y} = a^{[2]} = \\sigma(Z^{ [2]})\\tag{4}$$\n",
        "For one example;\n",
        "$$y^{(i)}_{prediction} = \\begin{cases} 1 & \\mbox{if } a^{[2](i)} > 0.5 \\\\ 0 & \\mbox{otherwise } \\end{cases}\\tag{5}$$\n",
        "\n",
        "Given the predictions on all the examples, the cost $J$ can be computed as follows: \n",
        "$$J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large\\left(\\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right)  \\large  \\right) \\small \\tag{6}$$\n",
        "\n",
        "\n",
        "\n",
        "When we multiply matrices, as in the product $W^{[1]} X$ and $W^{[2]} A^{[1]}$ , the dimensions of those matrices have to be correct in order for the product to be possible. Thatâ€™s why itâ€™s essential to set the dimensions of our weights and biases matrices right.\n",
        "\n",
        "- W1: **Shape is (n_h, n_x)** where n_h is the number of hidden units of that layer and n_x is the number of features/rows of the previous layer (in this case X, our input data).\n",
        "- b1: **Shape is (n_h, 1)** where n_h is the number of hidden nodes of that layer. It always has the same number of rows as W1 and a single column.\n",
        "- W2: **Shape is (n_y, n_h)** where n_y is the number of units of that output layer and n_h is the number of nodes of the previous layer (hidden layer in our case).\n",
        "- b2: **Shape is (n_y, 1)** where n_y is the number of nodes of that layer. It always has the same number of rows as W2 and a single column.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BywKAGn9veQj"
      },
      "source": [
        "**The steps you have to implement are:**\n",
        "\n",
        "1. Retrieve each parameter from the dictionary \"parameters\" (which is the output of `initialize_parameters()`) by using `parameters[\"..\"]`.\n",
        "2. Implement Forward Propagation. Compute $Z^{[1]}, A^{[1]}, Z^{[2]}$ and $A^{[2]}$ (the vector of all your predictions on all the examples in the training set)\n",
        "3. Values needed in the backpropagation are stored in \"`cache`\". The `cache` will be given as an input to the backpropagation function.\n",
        "\n",
        "**Note**: \n",
        "\n",
        "- You can use the function `sigmoid()`. It is built-in (imported) in the notebook.\n",
        "- You can use the function `np.tanh()`. It is part of the numpy library.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_mqDPXl0nA2"
      },
      "source": [
        "def forward_propagation(X, parameters):\n",
        "    \"\"\"\n",
        "    Argument:\n",
        "    X -- input data of size (n_x, m)\n",
        "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
        "    \n",
        "    Returns:\n",
        "    A2 -- The sigmoid output of the second activation\n",
        "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
        "    \"\"\"\n",
        "    # Retrieve each parameter from the dictionary \"parameters\"\n",
        "    ### START CODE HERE ### (â‰ˆ 4 lines of code)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Implement Forward Propagation to calculate A2 (probabilities)\n",
        "    ### START CODE HERE ### (â‰ˆ 4 lines of code)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    assert(A2.shape == (1, X.shape[1]))\n",
        "    \n",
        "    cache = {\"Z1\": Z1,\n",
        "             \"A1\": A1,\n",
        "             \"Z2\": Z2,\n",
        "             \"A2\": A2}\n",
        "    \n",
        "    return A2, cache"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPAk-PuY0nA3"
      },
      "source": [
        "X_assess, parameters = forward_propagation_test_case()\n",
        "A2, cache = forward_propagation(X_assess, parameters)\n",
        "\n",
        "# Note: we use the mean here just to make sure that your output matches ours. \n",
        "print(np.mean(cache['Z1']) ,np.mean(cache['A1']),np.mean(cache['Z2']),np.mean(cache['A2']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABIskEje0nA5"
      },
      "source": [
        "**Expected Output**:\n",
        "<table style=\"width:50%\">\n",
        "  <tr>\n",
        "    <td> 0.262818640198 0.091999045227 -1.30766601287 0.212877681719 </td> \n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuAZwn6c0nA6"
      },
      "source": [
        "####**Cost Function**\n",
        "\n",
        "Now that you have computed $A^{[2]}$ ( \"`A2`\" in the code), which contains $a^{[2](i)}$ for every example, you can compute the cost function as follows:\n",
        "\n",
        "$$J = - \\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(} \\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right) \\large{)} \\small\\tag{13}$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEFj4QUK0nA6"
      },
      "source": [
        "def compute_cost(A2, Y, parameters):\n",
        "    \"\"\"\n",
        "    Computes the cross-entropy cost given in equation (13)\n",
        "    \n",
        "    Arguments:\n",
        "    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
        "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
        "    parameters -- python dictionary containing your parameters W1, b1, W2 and b2\n",
        "    \n",
        "    Returns:\n",
        "    cost -- cross-entropy cost given equation (13)\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "     # retrieve the number of example\n",
        "   ### START CODE HERE ### (â‰ˆ 1 lines of code)\n",
        "  \n",
        "   ### END CODE HERE ###\n",
        "\n",
        "    # Compute the cross-entropy cost\n",
        "    ### START CODE HERE ### (â‰ˆ 1 lines of code)\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    cost = float(np.squeeze(cost))  # makes sure cost is the dimension we expect. \n",
        "                                    # E.g., turns [[17]] into 17 \n",
        "    assert(isinstance(cost, float))\n",
        "    \n",
        "    return cost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXHwvZhG0nA-"
      },
      "source": [
        "A2, Y_assess, parameters = compute_cost_test_case()\n",
        "\n",
        "print(\"cost = \" + str(compute_cost(A2, Y_assess, parameters)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkKxy9AG0nBA"
      },
      "source": [
        "**Expected Output**:\n",
        "<table style=\"width:20%\">\n",
        "  <tr>\n",
        "    <td>**cost**</td>\n",
        "    <td> 0.6930587610394646 </td> \n",
        "  </tr>\n",
        "  \n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjPSxsUT0nBA"
      },
      "source": [
        "###4.4 Backward Propagation\n",
        "\n",
        "Backpropagation is usually the hardest (most mathematical) part in deep learning. To help you, here again is the slide from the lecture on backpropagation. You'll want to use the six equations on the right of this slide, since you are building a vectorized implementation.  \n",
        "\n",
        "\n",
        "$ \\partial Z^{[2]}  = A^{[2]} - Y$\n",
        "\n",
        "$\\partial W^{[2]}  = \\frac{1}{m} \\partial Z^{[2]} A^{[1]T}$\n",
        "\n",
        "$\\partial b^{[2]} = \\frac{1}{m} np.sum (\\partial Z^{[2]}, axis=1, keepdims=True)$\n",
        "\n",
        "$ \\partial Z^{[1]}  =  W^{[2]T} \\partial Z^{[2]} *  g^{[1]'}(Z^{[1]})$\n",
        "\n",
        "$\\partial W^{[1]}  = \\frac{1}{m} \\partial Z^{[1]} X^{T}$\n",
        "\n",
        "$\\partial b^{[1]} = \\frac{1}{m} np.sum (\\partial Z^{[1]}, axis=1, keepdims=True)$\n",
        "\n",
        "- Note that $*$ denotes elementwise multiplication.\n",
        "- The notation you will use in coding:\n",
        "    - dW1 = $\\frac{\\partial \\mathcal{J} }{ \\partial W_1 }$\n",
        "    - db1 = $\\frac{\\partial \\mathcal{J} }{ \\partial b_1 }$\n",
        "    - dW2 = $\\frac{\\partial \\mathcal{J} }{ \\partial W_2 }$\n",
        "    - db2 = $\\frac{\\partial \\mathcal{J} }{ \\partial b_2 }$\n",
        "    \n",
        "- To compute dZ1 you'll need to compute $g^{[1]'}(Z^{[1]})$. Since $g^{[1]}(.)$ is the tanh activation function, so for $a = g^{[1]}(z)$ then $g^{[1]'}(z) = 1-a^2$. So you can compute $g^{[1]'}(Z^{[1]})$ as $(1 - A^{[1]^2})$\n",
        "\n",
        "**HINT:** Use np.power function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlMvuO9k0nBA"
      },
      "source": [
        "def backward_propagation(parameters, cache, X, Y):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation using the instructions above.\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing our parameters \n",
        "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
        "    X -- input data of shape (2, number of examples)\n",
        "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
        "    \n",
        "    Returns:\n",
        "    grads -- python dictionary containing your gradients with respect to different parameters\n",
        "    \"\"\"\n",
        "    # retrieve the number of example\n",
        "    ### START CODE HERE ### (â‰ˆ 1 lines of code)\n",
        "  \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n",
        "    ### START CODE HERE ### (â‰ˆ 2 lines of code)\n",
        "   \n",
        "\n",
        "    ### END CODE HERE ###\n",
        "        \n",
        "    # Retrieve also A1 and A2 from dictionary \"cache\".\n",
        "    ### START CODE HERE ### (â‰ˆ 2 lines of code)\n",
        "  \n",
        "\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Backward propagation: calculate dZ2, dW2, db2, dZ1, dW1, db1. \n",
        "    ### START CODE HERE ### (â‰ˆ 6 lines of code, corresponding to 6 equations on slide above)\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    grads = {\"dW1\": dW1,\n",
        "             \"db1\": db1,\n",
        "             \"dW2\": dW2,\n",
        "             \"db2\": db2}\n",
        "    \n",
        "    return grads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypYlW3PR0nBC"
      },
      "source": [
        "parameters, cache, X_assess, Y_assess = backward_propagation_test_case()\n",
        "\n",
        "grads = backward_propagation(parameters, cache, X_assess, Y_assess)\n",
        "print (\"dW1 = \"+ str(grads[\"dW1\"]))\n",
        "print (\"db1 = \"+ str(grads[\"db1\"]))\n",
        "print (\"dW2 = \"+ str(grads[\"dW2\"]))\n",
        "print (\"db2 = \"+ str(grads[\"db2\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGWcsBok0nBE"
      },
      "source": [
        "**Expected output**:\n",
        "\n",
        "\n",
        "\n",
        "<table style=\"width:80%\">\n",
        "  <tr>\n",
        "    <td>**dW1**</td>\n",
        "    <td> [[ 0.00301023 -0.00747267]\n",
        " [ 0.00257968 -0.00641288]\n",
        " [-0.00156892  0.003893  ]\n",
        " [-0.00652037  0.01618243]] </td> \n",
        "  </tr>\n",
        "  \n",
        "  <tr>\n",
        "    <td>**db1**</td>\n",
        "    <td>  [[ 0.00176201]\n",
        " [ 0.00150995]\n",
        " [-0.00091736]\n",
        " [-0.00381422]] </td> \n",
        "  </tr>\n",
        "  \n",
        "  <tr>\n",
        "    <td>**dW2**</td>\n",
        "    <td> [[ 0.00078841  0.01765429 -0.00084166 -0.01022527]] </td> \n",
        "  </tr>\n",
        "  \n",
        "\n",
        "  <tr>\n",
        "    <td>**db2**</td>\n",
        "    <td> [[-0.16655712]] </td> \n",
        "  </tr>\n",
        "  \n",
        "</table>  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74-Dujvd0nBE"
      },
      "source": [
        "### 4.5 Gradient descent\n",
        "\n",
        "**Gradient descent (GD)** constructs a sequence of weight vectors $\\mathbf{w}^{(0)},\\mathbf{w}^{(1)},\\ldots$ such that the loss function tends toward the minimum loss. The GD algorithm requires the specification of a suitable learning rate  $\\alpha$ and initial guess $\\mathbf{W}^{[l]}$ and then repeating the gradient step for a sufficient number of iterations. One possible stopping criterion is to use a fixed number of iterations. Another option is to monitor the loss function and stop if consecutive iterates do not result in any significant decrease. \n",
        "\n",
        "A key challenge in the use of GD is to find a good choice for the learning rate $\\alpha$. If the learning rate is too small (plot below), the GD steps make too little progress and thus requires an excessive number of iterations to get close to the optimum weight vector. \n",
        "\n",
        "Run the code below to see the example of the gradient descent algorithm with a good learning rate (converging). Images courtesy of Adam Harley.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Alh_gxIs6FZh"
      },
      "source": [
        "from IPython.display import Image \n",
        "Image(open('/content/drive/My Drive/Deep_learning_unit/Assignment3/images/sgd.gif','rb').read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfhPBI5VQodb"
      },
      "source": [
        "Conversely, if the learning rate is too high (right plot below), it is possible that as GD iterates $\\mathbf{w}$ will \"overshoot\" the minimum and climb up the loss function on the other side of the minimum (GD diverges). \n",
        "\n",
        "An example of the gradient descent algorithm with a bad learning rate (diverging) is shown below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EE_tDOBGQW18"
      },
      "source": [
        "from IPython.display import Image \n",
        "Image(open('/content/drive/My Drive/Deep_learning_unit/Assignment3/images/sgd_bad.gif','rb').read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skFApj106qTk"
      },
      "source": [
        "**Update Parameters**\n",
        "\n",
        "Now use gradient descent to update the parameters (W1, b1, W2, b2) using the calcilated derivatives (dW1, db1, dW2, db2) in backprop.\n",
        "\n",
        "**You can use this formula to update your parameters:**\n",
        "\n",
        " $ \\theta = \\theta - \\alpha * \\partial \\theta $ where $\\alpha$ is the learning rate and $\\theta$ represents a parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y72iBykX0nBE"
      },
      "source": [
        "def update_parameters(parameters, grads, learning_rate = 1.2):\n",
        "    \"\"\"\n",
        "    Updates parameters using the gradient descent update rule given above\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters \n",
        "    grads -- python dictionary containing your gradients \n",
        "    \n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your updated parameters \n",
        "    \"\"\"\n",
        "    # Retrieve each parameter from the dictionary \"parameters\"\n",
        "    ### START CODE HERE ### (â‰ˆ 4 lines of code)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Retrieve each gradient from the dictionary \"grads\"\n",
        "    ### START CODE HERE ### (â‰ˆ 4 lines of code)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ## END CODE HERE ###\n",
        "    \n",
        "    # Update rule for each parameter\n",
        "    ### START CODE HERE ### (â‰ˆ 4 lines of code)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "    \n",
        "    return parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "7DndwqvU0nBG"
      },
      "source": [
        "parameters, grads = update_parameters_test_case()\n",
        "parameters = update_parameters(parameters, grads)\n",
        "\n",
        "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
        "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
        "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
        "print(\"b2 = \" + str(parameters[\"b2\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bX8crob0nBJ"
      },
      "source": [
        "**Expected Output**:\n",
        "\n",
        "\n",
        "<table style=\"width:80%\">\n",
        "  <tr>\n",
        "    <td>**W1**</td>\n",
        "    <td> [[-0.00643025  0.01936718]\n",
        " [-0.02410458  0.03978052]\n",
        " [-0.01653973 -0.02096177]\n",
        " [ 0.01046864 -0.05990141]]</td> \n",
        "  </tr>\n",
        "  \n",
        "  <tr>\n",
        "    <td>**b1**</td>\n",
        "    <td> [[ -1.02420756e-06]\n",
        " [  1.27373948e-05]\n",
        " [  8.32996807e-07]\n",
        " [ -3.20136836e-06]]</td> \n",
        "  </tr>\n",
        "  \n",
        "  <tr>\n",
        "    <td>**W2**</td>\n",
        "    <td> [[-0.01041081 -0.04463285  0.01758031  0.04747113]] </td> \n",
        "  </tr>\n",
        "  \n",
        "\n",
        "  <tr>\n",
        "    <td>**b2**</td>\n",
        "    <td> [[ 0.00010457]] </td> \n",
        "  </tr>\n",
        "  \n",
        "</table>  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_c0kgVe0nBJ"
      },
      "source": [
        "###4.6 - Neural network model  \n",
        "\n",
        "Now is the time to integrate all the functions created above in the right order into a single function called `nn_model()`.\n",
        "\n",
        "The functions which need to be used are:\n",
        "- initialize_parameters(n_x, n_h, n_y)\n",
        "- forward_propagation(X, parameters)\n",
        "- compute_cost(A2, Y, parameters)\n",
        "- backward_propagation(parameters, cache, X, Y)\n",
        "- update_parameters(parameters, grads, learning_rate = 1.2)\n",
        "\n",
        "Call the above functions in the right order with right parameters in the code below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdtDoeSy0nBJ"
      },
      "source": [
        "def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    X -- dataset of shape (2, number of examples)\n",
        "    Y -- labels of shape (1, number of examples)\n",
        "    n_h -- size of the hidden layer\n",
        "    num_iterations -- Number of iterations in gradient descent loop\n",
        "    print_cost -- if True, print the cost every 1000 iterations\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Retrieve the values of n_x and n_y from X and Y\n",
        "    ### START CODE HERE ### (â‰ˆ 2 lines of code)\n",
        "  \n",
        "\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Initialize parameters\n",
        "    ### START CODE HERE ### (â‰ˆ 1 line of code)\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Loop (gradient descent)\n",
        "\n",
        "    for i in range(0, num_iterations):\n",
        "        \n",
        "        # Call all the 4 functions listed above in right order\n",
        "        ### START CODE HERE ### (â‰ˆ 4 lines of code)\n",
        "     \n",
        "\n",
        "\n",
        "\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "        # Print the cost every 1000 iterations\n",
        "        if print_cost and i % 1000 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "\n",
        "    return parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lW-E2Ppw0nBL"
      },
      "source": [
        "X_assess, Y_assess = nn_model_test_case()\n",
        "parameters = nn_model(X_assess, Y_assess, 4, num_iterations=10000, print_cost=True)\n",
        "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
        "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
        "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
        "print(\"b2 = \" + str(parameters[\"b2\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMK5JIwn0nBQ"
      },
      "source": [
        "**Expected Output**:\n",
        "\n",
        "<table style=\"width:90%\">\n",
        "<tr> \n",
        "    <td> \n",
        "        **cost after iteration 0**\n",
        "    </td>\n",
        "    <td> \n",
        "        0.692739\n",
        "    </td>\n",
        "</tr>\n",
        "  <tr>\n",
        "    <td>**W1**</td>\n",
        "    <td> [[-0.65848169  1.21866811]\n",
        " [-0.76204273  1.39377573]\n",
        " [ 0.5792005  -1.10397703]\n",
        " [ 0.76773391 -1.41477129]]</td> \n",
        "  </tr>  \n",
        "  <tr>\n",
        "    <td>**b1**</td>\n",
        "    <td> [[ 0.287592  ]\n",
        " [ 0.3511264 ]\n",
        " [-0.2431246 ]\n",
        " [-0.35772805]] </td> \n",
        "  </tr>\n",
        "  \n",
        "  <tr>\n",
        "    <td>**W2**</td>\n",
        "    <td> [[-2.45566237 -3.27042274  2.00784958  3.36773273]] </td> \n",
        "  </tr>\n",
        "  \n",
        "\n",
        "  <tr>\n",
        "    <td>**b2**</td>\n",
        "    <td> [[ 0.20459656]] </td> \n",
        "  </tr>\n",
        "  \n",
        "</table>  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zt5phIxd0nBQ"
      },
      "source": [
        "### 4.7 Predictions\n",
        "\n",
        " Use your model to predict by building predict().\n",
        "Use forward propagation to predict results.\n",
        "\n",
        " predictions = $y_{prediction}$ = \\begin{cases}\n",
        "      1 & \\text{if}\\ activation > 0.5 \\\\\n",
        "      0 & \\text{otherwise}\n",
        "    \\end{cases}  \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GosXSK650nBQ"
      },
      "source": [
        "# GRADED FUNCTION: predict\n",
        "\n",
        "def predict(parameters, X):\n",
        "    \"\"\"\n",
        "    Using the learned parameters, predicts a class for each example in X\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters \n",
        "    X -- input data of size (n_x, m)\n",
        "    \n",
        "    Returns\n",
        "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
        "    \"\"\"\n",
        "    \n",
        "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
        "    ### START CODE HERE ### (â‰ˆ 2 lines of code)\n",
        "  \n",
        "   \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "HqFM__I-0nBS"
      },
      "source": [
        "parameters, X_assess = predict_test_case()\n",
        "\n",
        "predictions = predict(parameters, X_assess)\n",
        "print(\"predictions mean = \" + str(np.mean(predictions)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0x2VItK0nBT"
      },
      "source": [
        "**Expected Output**: \n",
        "\n",
        "\n",
        "<table style=\"width:40%\">\n",
        "  <tr>\n",
        "    <td>**predictions mean**</td>\n",
        "    <td> 0.666666666667 </td> \n",
        "  </tr>\n",
        "  \n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BwTpAN-0nBT"
      },
      "source": [
        "**Run model**\n",
        "\n",
        "It is time to run the model and see how it performs on a moons dataset. Run the following code to test your model with a single hidden layer of $n_h$ hidden units."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "BbtA5K7m0nBT"
      },
      "source": [
        "# Build a model with a n_h-dimensional hidden layer\n",
        "# Call the nn_model function for your dataset\n",
        " ### START CODE HERE ### (â‰ˆ 1 lines of code)\n",
        "  \n",
        " ### END CODE HERE ###\n",
        " \n",
        "# Plot the decision boundary\n",
        "plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)\n",
        "plt.title(\"Decision Boundary for hidden layer size \" + str(4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M15i72FF0nBU"
      },
      "source": [
        "**Expected Output**:\n",
        "\n",
        "<table style=\"width:40%\">\n",
        "  <tr>\n",
        "    <td>**Cost after iteration 9000**</td>\n",
        "    <td> 0.069172 </td> \n",
        "  </tr>\n",
        "  \n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95DNPao-0nBV"
      },
      "source": [
        "# Print accuracy\n",
        "predictions = predict(parameters, X)\n",
        "print ('Accuracy: %d' % float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100) + '%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmPnGt-t0nBW"
      },
      "source": [
        "**Expected Output**: \n",
        "\n",
        "<table style=\"width:15%\">\n",
        "  <tr>\n",
        "    <td>**Accuracy**</td>\n",
        "    <td> 97% </td> \n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n95tWJcz0nBW"
      },
      "source": [
        "Accuracy is really high compared to Logistic Regression. The model has learnt the patterns of the dataset! Neural networks are able to learn even highly non-linear decision boundaries, unlike logistic regression. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opCUoe-w0nBW"
      },
      "source": [
        "### 5 - Further experiments\n",
        "\n",
        "Let's try to change the hidden layer sizes and see how our model performs. Run the following code and you will observe different behaviors of the model for various hidden layer sizes. \n",
        "\n",
        "**Note** It may take upto 2 minutes to run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "wWDQL-7X0nBW"
      },
      "source": [
        "plt.figure(figsize=(16, 32))\n",
        "hidden_layer_sizes = [1, 2, 3, 4, 5, 10, 30]\n",
        "for i, n_h in enumerate(hidden_layer_sizes):\n",
        "    plt.subplot(5, 2, i+1)\n",
        "    plt.title('Hidden Layer of size %d' % n_h)\n",
        "    parameters = nn_model(X, Y, n_h, num_iterations = 5000)\n",
        "    plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)\n",
        "    predictions = predict(parameters, X)\n",
        "    accuracy = float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100)\n",
        "    print (\"Accuracy for {} hidden units: {} %\".format(n_h, accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8WJsaj00nBX"
      },
      "source": [
        "**Interpretation**:\n",
        "- The larger models (with more hidden units) are able to fit the training set better, until eventually the largest models starts behaving worst. \n",
        "- The best hidden layer size seems to be around n_h = 4. Indeed, a value around here seems to  fits the data well without also incurring noticeable overfitting. "
      ]
    }
  ]
}